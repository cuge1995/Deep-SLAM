# Deep-SLAM
a list of papers, code, and other resources focus on deep learning SLAM sysytem

## Camera
* DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras [[code]](https://github.com/princeton-vl/DROID-SLAM)[[paper]](https://arxiv.org/pdf/2108.10869)
* Deeptam: Deep tracking and mapping [no code]()[[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Huizhong_Zhou_DeepTAM_Deep_Tracking_ECCV_2018_paper.pdf) `ECCV 2018`
* Beyond tracking: Selecting memory and refining poses for deep visual odometry [no code]()[[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xue_Beyond_Tracking_Selecting_Memory_and_Refining_Poses_for_Deep_Visual_CVPR_2019_paper.pdf) `CVPR 2019`
* Sequential adversarial learning for self-supervised deep visual odometry [no code]()[[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Sequential_Adversarial_Learning_for_Self-Supervised_Deep_Visual_Odometry_ICCV_2019_paper.pdf)
* D2VO: Monocular Deep Direct Visual Odometry [no code]()[[paper]](http://ras.papercept.net/images/temp/IROS/files/2025.pdf) `IROS 2020`
* Deepfactors: Real-time probabilistic dense monocular slam [no code]()[[paper]](https://arxiv.org/pdf/2001.05049) `IEEE Robotics and Automation Letters, 2020`




## LiDAR
* Lo-net: Deep real-time lidar odometry [no code][[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_LO-Net_Deep_Real-Time_Lidar_Odometry_CVPR_2019_paper.pdf) `CVPR 2019`
